{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 20:00:47.175878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/jhsu12/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame: 2, pred:  \n",
      "frame: 4, pred: 1\n",
      "frame: 6, pred: 1\n",
      "frame: 8, pred: 121\n",
      "frame: 10, pred: 1212\n",
      "frame: 12, pred: 1212 \n",
      "frame: 14, pred: 12a\n",
      "frame: 16, pred: 123\n",
      "frame: 18, pred: 12 3\n",
      "frame: 20, pred: 12 34\n",
      "frame: 22, pred: 12 a34\n",
      "frame: 24, pred: 12 a345\n",
      "frame: 26, pred: 12 a345\n",
      "frame: 28, pred: 12 a345m\n",
      "frame: 30, pred: 12 a345m\n",
      "frame: 32, pred: 12a345 m\n",
      "frame: 34, pred: 12 a345 mv\n",
      "frame: 36, pred: 12a345 mv\n",
      "frame: 38, pred: 12a345 m21\n",
      "frame: 40, pred: 12a345 m21\n",
      "frame: 42, pred: 12a345 m21\n",
      "frame: 44, pred: 12a345 m21\n",
      "frame: 46, pred: 12a345 m21\n",
      "frame: 48, pred: 12a345 m21\n",
      "frame: 50, pred: 12a345 mv1\n",
      "frame: 52, pred: 12a345 m21c\n",
      "frame: 54, pred: 12a345 m21 c\n",
      "frame: 56, pred: 12a345 m21 c\n",
      "frame: 58, pred: 12a345 m21 c\n",
      "frame: 60, pred: 12a345 m21 c\n",
      "frame: 62, pred: 12a345 m21 c\n",
      "frame: 64, pred: 12a345 m21 c\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = pd.read_json('./results2nd/inference_args.json')\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    return pd.read_parquet(pq_path, columns=selected_columns['selected_columns'])\n",
    "    #return pd.read_parquet(pq_path)\n",
    "\n",
    "def predict(xyz_np, prediction_fn, rev_character_map):\n",
    "    \n",
    "\n",
    "    #pq_file = './output.parquet'\n",
    "    #xyz_np = load_relevant_data_subset(pq_file)\n",
    "    xyz_np = xyz_np.loc[:, selected_columns['selected_columns']]\n",
    "    xyz_np = xyz_np.astype('float32')\n",
    "    prediction = prediction_fn(inputs=xyz_np.values)\n",
    "\n",
    "   \n",
    "    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(prediction['outputs'], axis=1)])\n",
    "\n",
    "    return prediction_str\n",
    "\n",
    "def load_model():\n",
    "    interpreter = tf.lite.Interpreter('./results2nd/model.tflite')\n",
    "    found_signatures = list(interpreter.get_signature_list().keys())\n",
    "    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "    with open (\"./character_to_prediction_index.json\", \"r\") as f:\n",
    "        character_map = json.load(f)\n",
    "    rev_character_map = {j:i for i,j in character_map.items()}\n",
    "\n",
    "    return prediction_fn, rev_character_map\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "def create_frame_landmark_df(results, frame, cols):\n",
    "    landmarks = pd.DataFrame()\n",
    "   \n",
    "\n",
    "    landmarks.loc[0, cols] = np.nan\n",
    "    landmarks = landmarks.assign(frame=frame)\n",
    "    \n",
    "    if results.face_landmarks:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_face_{i}', f'y_face_{i}', f'z_face_{i}']] = [point.x, point.y, point.z]\n",
    "    if results.pose_landmarks:\n",
    "        for i, point in enumerate(results.pose_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_pose_{i}', f'y_pose_{i}', f'z_pose_{i}']] = [point.x, point.y, point.z]\n",
    "    if results.left_hand_landmarks:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_left_hand_{i}', f'y_left_hand_{i}', f'z_left_hand_{i}']] = [point.x, point.y, point.z]\n",
    "    if results.right_hand_landmarks:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_right_hand_{i}', f'y_right_hand_{i}', f'z_right_hand_{i}']] = [point.x, point.y, point.z]\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def do_capture_loop(cols):\n",
    "\n",
    "    # load model\n",
    "    prediction_fn, rev_character_map = load_model()\n",
    "    #try:\n",
    "    # For webcam input:\n",
    "    landmarks_all = []\n",
    "    text = \"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        frame = 0\n",
    "        while cap.isOpened():\n",
    "            frame += 1\n",
    "            \n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "                continue\n",
    "\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            \n",
    "            # Create landmark df\n",
    "            landmarks = create_frame_landmark_df(results, frame, cols)\n",
    "            landmarks_all.append(landmarks)\n",
    "            \n",
    "            if frame%2 == 0:\n",
    "                #landmarks = pd.concat(landmarks_all).reset_index(drop=True).to_parquet('output.parquet')\n",
    "                landmarks = pd.concat(landmarks_all).reset_index(drop=True)\n",
    "                # predict\n",
    "                predict_str = predict(landmarks, prediction_fn, rev_character_map)\n",
    "                text = predict_str\n",
    "                print(f\"frame: {frame}, pred: {predict_str}\")\n",
    "                \n",
    "            \n",
    "            # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            '''\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "            '''\n",
    "            \n",
    "            # Rectangle properties\n",
    "            rect_x = 0\n",
    "            rect_y = 500  # Adjust this value to control the position of the rectangle\n",
    "            rect_width = 400\n",
    "            rect_height = 200\n",
    "            rect_color = (0, 0, 0)  # Black color in BGR format\n",
    "\n",
    "\n",
    "            # Define the text and background colors\n",
    "            text_color = (255, 255, 255)  # White text in BGR format\n",
    "            background_color = (0, 0, 0)  # Black background in BGR format\n",
    "\n",
    "            # Add text to the frame with a black background\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1\n",
    "            thickness = 1\n",
    "\n",
    "            # Calculate text position to center it in the rectangle\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "            text_x = (rect_width - text_width) // 2 + rect_x\n",
    "            text_y = (rect_height - text_height) // 2 + rect_y + text_height\n",
    "\n",
    "\n",
    "            # Create a black rectangle as a background for the text\n",
    "       \n",
    "            cv2.rectangle(image, (rect_x, rect_y), (rect_x + rect_width, rect_y + rect_height), rect_color, -1)\n",
    "\n",
    "            # Put the white text on the black background\n",
    "            frame_with_text = cv2.putText(image, text, (text_x, text_y), font, font_scale, text_color, thickness)\n",
    "        \n",
    "            # Flip the image horizontally for a selfie-view display.\n",
    "            #cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "\n",
    "            \n",
    "\n",
    "            cv2.imshow('MediaPipe Holistic', frame_with_text)\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    #except:\n",
    "    #   return landmarks_all\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return landmarks_all\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pq_file = './input/train_landmarks/1255240050.parquet'\n",
    "    xyz = pd.read_parquet(pq_file)\n",
    "    do_capture_loop(xyz.columns[1:])\n",
    "    #print(landmarks)\n",
    "    #landmarks = pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
