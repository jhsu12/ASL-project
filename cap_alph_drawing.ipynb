{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 11:36:14.998760: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/jhsu12/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame: 4, pred: w\n",
      "frame: 8, pred: 11\n",
      "frame: 12, pred: +21-\n",
      "frame: 16, pred: +22-2\n",
      "frame: 20, pred: 12 p\n",
      "frame: 24, pred: 12 w\n",
      "frame: 28, pred: 1266 \n",
      "frame: 32, pred: 1226\n",
      "frame: 36, pred: +226\n",
      "frame: 40, pred: +216\n",
      "frame: 44, pred: +226\n",
      "frame: 48, pred: +226\n",
      "frame: 52, pred: +226\n",
      "frame: 56, pred: +226-11\n",
      "frame: 60, pred: +226-11\n",
      "frame: 64, pred: +226-11-1\n",
      "frame: 68, pred: +266-11-1\n",
      "frame: 72, pred: +26-11-10-1\n",
      "frame: 76, pred: +26-11-0-1\n",
      "frame: 80, pred: +26-11-01-1\n",
      "frame: 84, pred: +26-11-00-1\n",
      "frame: 88, pred: +26-11-10-1\n",
      "frame: 92, pred: +26-11-10-1\n",
      "frame: 96, pred: +26-11-00-1\n",
      "frame: 100, pred: +26-11-00-1\n",
      "frame: 104, pred: +26-11-00-1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jhsu12/Desktop/專題/ASL/cap_alph_drawing.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 152>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m pq_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./input/train_landmarks/1255240050.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m xyz \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(pq_file)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m do_capture_loop(xyz\u001b[39m.\u001b[39;49mcolumns[\u001b[39m1\u001b[39;49m:])\n",
      "\u001b[1;32m/Users/jhsu12/Desktop/專題/ASL/cap_alph_drawing.ipynb Cell 2\u001b[0m line \u001b[0;36mdo_capture_loop\u001b[0;34m(cols)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m landmarks \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(landmarks_all)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m predict_str \u001b[39m=\u001b[39m predict(landmarks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m text \u001b[39m=\u001b[39m predict_str\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mframe: \u001b[39m\u001b[39m{\u001b[39;00mframe\u001b[39m}\u001b[39;00m\u001b[39m, pred: \u001b[39m\u001b[39m{\u001b[39;00mpredict_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/jhsu12/Desktop/專題/ASL/cap_alph_drawing.ipynb Cell 2\u001b[0m line \u001b[0;36mpredict\u001b[0;34m(xyz_np)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m xyz_np \u001b[39m=\u001b[39m xyz_np\u001b[39m.\u001b[39mloc[:, selected_columns[\u001b[39m'\u001b[39m\u001b[39mselected_columns\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m xyz_np \u001b[39m=\u001b[39m xyz_np\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m prediction \u001b[39m=\u001b[39m prediction_fn(inputs\u001b[39m=\u001b[39;49mxyz_np\u001b[39m.\u001b[39;49mvalues)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m./character_to_prediction_index.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jhsu12/Desktop/%E5%B0%88%E9%A1%8C/ASL/cap_alph_drawing.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         character_map \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:257\u001b[0m, in \u001b[0;36mSignatureRunner.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m input_name, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    254\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter_wrapper\u001b[39m.\u001b[39mSetTensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs[input_name], value,\n\u001b[1;32m    255\u001b[0m                                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subgraph_index)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter_wrapper\u001b[39m.\u001b[39;49mInvoke(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_subgraph_index)\n\u001b[1;32m    258\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    259\u001b[0m \u001b[39mfor\u001b[39;00m output_name, output_index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = pd.read_json('./results2nd/inference_args.json')\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    return pd.read_parquet(pq_path, columns=selected_columns['selected_columns'])\n",
    "    #return pd.read_parquet(pq_path)\n",
    "\n",
    "def predict(xyz_np):\n",
    "    interpreter = tf.lite.Interpreter('./results2nd/model.tflite')\n",
    "    found_signatures = list(interpreter.get_signature_list().keys())\n",
    "    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "    #pq_file = './output.parquet'\n",
    "    #xyz_np = load_relevant_data_subset(pq_file)\n",
    "    xyz_np = xyz_np.loc[:, selected_columns['selected_columns']]\n",
    "    xyz_np = xyz_np.astype('float32')\n",
    "    prediction = prediction_fn(inputs=xyz_np.values)\n",
    "\n",
    "    with open (\"./character_to_prediction_index.json\", \"r\") as f:\n",
    "            character_map = json.load(f)\n",
    "    rev_character_map = {j:i for i,j in character_map.items()}\n",
    "    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(prediction['outputs'], axis=1)])\n",
    "\n",
    "    return prediction_str\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "def create_frame_landmark_df(results, frame, cols):\n",
    "    landmarks = pd.DataFrame()\n",
    "   \n",
    "\n",
    "    landmarks.loc[0, cols] = np.nan\n",
    "    landmarks = landmarks.assign(frame=frame)\n",
    "    \n",
    "    if results.face_landmarks:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_face_{i}', f'y_face_{i}', f'z_face_{i}']] = [point.x, point.y, point.z]\n",
    "    if results.pose_landmarks:\n",
    "        for i, point in enumerate(results.pose_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_pose_{i}', f'y_pose_{i}', f'z_pose_{i}']] = [point.x, point.y, point.z]\n",
    "    if results.left_hand_landmarks:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_left_hand_{i}', f'y_left_hand_{i}', f'z_left_hand_{i}']] = [point.x, point.y, point.z]\n",
    "    if results.right_hand_landmarks:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            landmarks.loc[0, [f'x_right_hand_{i}', f'y_right_hand_{i}', f'z_right_hand_{i}']] = [point.x, point.y, point.z]\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def do_capture_loop(cols):\n",
    "    #try:\n",
    "    # For webcam input:\n",
    "    landmarks_all = []\n",
    "    text = \"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        frame = 0\n",
    "        while cap.isOpened():\n",
    "            frame += 1\n",
    "            \n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "                continue\n",
    "\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            \n",
    "            # Create landmark df\n",
    "            landmarks = create_frame_landmark_df(results, frame, cols)\n",
    "            landmarks_all.append(landmarks)\n",
    "            \n",
    "            if frame%4 == 0:\n",
    "                #landmarks = pd.concat(landmarks_all).reset_index(drop=True).to_parquet('output.parquet')\n",
    "                landmarks = pd.concat(landmarks_all).reset_index(drop=True)\n",
    "                # predict\n",
    "                predict_str = predict(landmarks)\n",
    "                text = predict_str\n",
    "                print(f\"frame: {frame}, pred: {predict_str}\")\n",
    "                \n",
    "            \n",
    "            # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            '''\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "            '''\n",
    "            \n",
    "            # Rectangle properties\n",
    "            rect_x = 0\n",
    "            rect_y = 500  # Adjust this value to control the position of the rectangle\n",
    "            rect_width = 400\n",
    "            rect_height = 200\n",
    "            rect_color = (0, 0, 0)  # Black color in BGR format\n",
    "\n",
    "\n",
    "            # Define the text and background colors\n",
    "            text_color = (255, 255, 255)  # White text in BGR format\n",
    "            background_color = (0, 0, 0)  # Black background in BGR format\n",
    "\n",
    "            # Add text to the frame with a black background\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1\n",
    "            thickness = 1\n",
    "\n",
    "            # Calculate text position to center it in the rectangle\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "            text_x = (rect_width - text_width) // 2 + rect_x\n",
    "            text_y = (rect_height - text_height) // 2 + rect_y + text_height\n",
    "\n",
    "\n",
    "            # Create a black rectangle as a background for the text\n",
    "       \n",
    "            cv2.rectangle(image, (rect_x, rect_y), (rect_x + rect_width, rect_y + rect_height), rect_color, -1)\n",
    "\n",
    "            # Put the white text on the black background\n",
    "            frame_with_text = cv2.putText(image, text, (text_x, text_y), font, font_scale, text_color, thickness)\n",
    "        \n",
    "            # Flip the image horizontally for a selfie-view display.\n",
    "            #cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "\n",
    "            \n",
    "\n",
    "            cv2.imshow('MediaPipe Holistic', frame_with_text)\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    #except:\n",
    "    #   return landmarks_all\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return landmarks_all\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pq_file = './input/train_landmarks/1255240050.parquet'\n",
    "    xyz = pd.read_parquet(pq_file)\n",
    "    do_capture_loop(xyz.columns[1:])\n",
    "    #print(landmarks)\n",
    "    #landmarks = pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
